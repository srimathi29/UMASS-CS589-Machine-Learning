# -*- coding: utf-8 -*-
"""hw8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WilDj0Rd0p_H8aQ1l68hzSB7NS4soP2u
"""

import numpy as np
import pandas as pd
import jax
from jax import grad
import jax.numpy as jnp
from sklearn.datasets import fetch_openml

import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')
datapath = '/content/drive/MyDrive/'
#datapath = "./"

# model
def model(x,w):
    a = w[0] + jnp.dot(x.T, w[1:])
    return a.T

# the softmax cost function
def softmax(w,x,y, lam):
	cost = jnp.sum(jnp.log( 1 + jnp.exp(-y*model(x,w)))) + jnp.sum(jnp.abs(w[1:])) * lam
	return cost / float(y.size)

def gradient_descent(x ,y , w, alpha, iterations, cost_func, lam):
	cost_history = []
	accuracy_history = []

	for i in range(iterations):
		gradient = grad(cost_func)(w,x,y, lam)
		w -= alpha * gradient
		cost = cost_func(w, x, y, lam)
		cost_history.append(cost)
		accuracy_history.append(calculate_metrics(x,y,w)[1])

		if i % 200 == 0 and i > 0:  # Check after a certain number of iterations
			if np.mean(cost_history[-100:-50]) - np.mean(cost_history[-50:]) < 1e-8:
				if alpha > 1e-3:
					alpha = alpha * 0.1
				else:
					print(f"Early stop at iteration {i}")
					break

	return w, cost_history, accuracy_history

def calculate_metrics(x, y, w_final):
	predictions = model(x, w_final)
	misclassifications = np.sum(np.sign(predictions) != y)
	accuracy = 100 - (misclassifications / y.size) * 100
	return misclassifications, accuracy

def train_test_split(*arrays, test_size=0.2, shuffle=True, rand_seed=0):
    # set the random state if provided
    np.random.seed(rand_seed)

    # initialize the split index
    array_len = len(arrays[0].T)
    split_idx = int(array_len * (1 - test_size))

    # initialize indices to the default order
    indices = np.arange(array_len)

    # shuffle the arrays if shuffle is True
    if shuffle:
        np.random.shuffle(indices)

    # Split the arrays
    result = []
    for array in arrays:
        if shuffle:
            array = array[:, indices]
        train = array[:, :split_idx]
        test = array[:, split_idx:]
        result.extend([train, test])

    return result

# define sigmoid
def sigmoid(t):
	return 1/(1 + jnp.exp(-t))

# K-Fold Cross-Validation
def k_fold_split(X, y, k):
    m = len(y)
    indices = np.arange(m)
    np.random.shuffle(indices)
    X_shuffled, y_shuffled = X[indices], y[indices]
    fold_size = m // k
    folds = [(X_shuffled[i*fold_size:(i+1)*fold_size], y_shuffled[i*fold_size:(i+1)*fold_size]) for i in range(k)]
    return folds

def k_fold_cross_validation(X, y, k, alpha, lambda_val, num_iters):
    validation_accuracies = np.zeros(k)
    fold_size = len(y.T) // k

    for fold in range(k):
        start_idx = fold * fold_size
        end_idx = (fold + 1) * fold_size
        x_val_fold = X[:, start_idx:end_idx]
        y_val_fold = y[:, start_idx:end_idx].flatten()

        x_train_fold = np.concatenate([X[:, :start_idx], X[:, end_idx:]], axis=1)
        y_train_fold = np.concatenate([y[:, :start_idx], y[:, end_idx:]], axis=1).flatten()

        mean = np.mean(x_train_fold, axis=1, keepdims=True)
        std = np.std(x_train_fold, axis=1, keepdims=True)
        x_train_fold = (x_train_fold - mean) / std
        w_init = np.zeros(x_train_fold.shape[0]+1)
        w_fold, cost_history, accuracy_history = gradient_descent(x_train_fold, y_train_fold, w_init, alpha, num_iters, softmax, lambda_val)

        misclassification, accuracy = calculate_metrics(x_val_fold, y_val_fold, w_fold)
        validation_accuracies[fold] = accuracy

    avg_validation_accuracy = np.mean(validation_accuracies)
    return avg_validation_accuracy, cost_history, accuracy_history, w_fold

def run_task1():
    csvname = datapath + 'new_gene_data.csv'
    data = np.loadtxt(csvname, delimiter=',')
    x = data[:-1, :]
    y = data[-1:, :]

    print(np.shape(x))  # (7128, 72)
    print(np.shape(y))  # (1, 72)

    np.random.seed(0)  # fix randomness
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, rand_seed=0)
    # initialize weights
    w = np.random.randn(x_train.shape[0] + 1, 1)

    # Hyperparameters
    learning_rates = [0.00001, 0.001, 0.1]
    lambda_values = [0.000001, 0.00001, 0.001]
    iterations = 1000
    k_values = [2,3,4,5]
    best_accuracy = 0
    w_final =[]

    # # K-Fold Cross-Validation
    ## use the k-fold cross validation function above to find the best hyperparameters
    for k in k_values:
        best_overall_accuracy = 0
        for alpha in learning_rates:
            for lambda_val in lambda_values:
                print(f"Alpha: {alpha}, Lambda: {lambda_val}, K: {k} Testing")
                avg_accuracy, cost_history, accuracy_history, w = k_fold_cross_validation(x_train, y_train, k, alpha, lambda_val, iterations)
                if avg_accuracy > best_overall_accuracy:
                    best_overall_accuracy = avg_accuracy
                    best_overall_params = {'k': k, 'learning_rate': alpha, 'lambda': lambda_val}
                    w_final = w

    print("Best Overall Hyperparameters:", best_overall_params)
    print("Best Overall Accuracy:", best_overall_accuracy)

    w_initt  = np.random.randn(x_train.shape[0] + 1, 1) * 0.01

    mean = jnp.mean(x_train, axis=1, keepdims=True)
    std = jnp.std(x_train, axis=1, keepdims=True)
    x_train_stdn = (x_train - mean) / std

    final_weights, cost_history, accuracy_history = gradient_descent(x_train, y_train, w_initt, best_overall_params['learning_rate'], iterations, softmax, best_overall_params['lambda'])

	 #plot cost history
    plt.figure()
    plt.plot(cost_history, label="Cost")
    plt.xlabel("Iterations")
    plt.ylabel("Cost")
    plt.legend()
    plt.show()

    train_misclassificationtr, train_accuracytr = calculate_metrics(x_train, y_train, final_weights)

    # Evaluate on test set
    mean = jnp.mean(x_test, axis=1, keepdims=True)
    std = jnp.std(x_test, axis=1, keepdims=True)
    x_test_stdn = (x_test - mean) / std
    test_misclassification, test_accuracy = calculate_metrics(x_test_stdn, y_test.flatten(), w_final)
    print("Test Accuracy:", test_accuracy)

    # # Identify top 5 influential genes
    top_genes = np.argsort(np.abs(final_weights),  axis=0)[-5:]
    print("Top 5 Influential Genes:", top_genes)

if __name__ == '__main__':
    run_task1()