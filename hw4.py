# -*- coding: utf-8 -*-
"""hw4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v4zP_Nj8M2c69zOPS9cm3H_3H5Id0pS3
"""

import numpy as np
from jax import grad
import jax.numpy as jnp
import matplotlib.pyplot as plt

#from google.colab import drive
#drive.mount('/content/drive')
#datapath = '/content/drive/MyDrive/'

datapath = "./"

# compute linear combination of input point
def model(x,w):
		a = w[0] + jnp.dot(x.T,w[1:])
		return a.T

# Define the softmax cost function
def softmax(w, x, y):
    cost = jnp.sum(jnp.log(1 + jnp.exp(-y * model(x, w))))
    return cost / float(y.size)


def calculate_metrics(x, y, w_final):
		predictions = model(x, w_final)
		misclassifications = np.sum(np.sign(predictions) != y)
		accuracy = 100 - (misclassifications / y.size) * 100
		return misclassifications, accuracy

def gradient_descent(x,y,w,alpha,iterations,cost_function):
	cost_history = []
	accuracy_history = []
	# print("gradient descent began")
	for i in range(iterations):
		# derivate of least squares is 1/m * (sum model(x,w) -y) x
		gradient = grad(cost_function)(w, x, y)
		w -= alpha * gradient
		cost = cost_function(w, x, y)
		cost_history.append(cost)
		accuracy_history.append(calculate_metrics(x,y,w)[1])

	return w, cost_history, accuracy_history
#################### Task 1 ###################

"""
Implementing the linear classification with Softmax cost;
verify the implementation is correct by achiving zero misclassification.
"""
def tanh(x):
	return jnp.tanh(x)

def run_task1():
	# load in data
	csvname = datapath + '2d_classification_data_v1.csv'
	data = np.loadtxt(csvname,delimiter = ',')

	# take input/output pairs from data
	x = data[:-1, :]
	y = data[-1:, :]

	print(np.shape(x)) # (1, 11)
	print(np.shape(y)) # (1, 11)

 # Initialize weights
	w = np.array([3.0, 3.0])

  # Hyperparameters
	max_iters = 2000
	alpha = 1.0

	w_final , cost_history, accuracy = gradient_descent(x, y, w, alpha, max_iters, cost_function=softmax)
 	# Plot cost history
	plt.figure()
	plt.plot(range(len(cost_history)), cost_history, label='Cost History', color='b')
	plt.title('Cost History Over Iterations')
	plt.xlabel('Iterations')
	plt.ylabel('Cost')
	plt.legend()
	plt.show()

	misclassifications, accuracy = calculate_metrics(x, y, w_final)
	print("Misclassifications:", misclassifications)
	print("Accuracy: {:.2f}%".format(accuracy))

	x_tanh = jnp.linspace(-0.5, 5.5, 400)[:, jnp.newaxis]
	y_tanh = tanh(w_final[0] + jnp.dot(x_tanh, w_final[1:]))
	plt.scatter(x,y, label="Original data")
	plt.plot(x_tanh, y_tanh, color='red', label="Fitted curve")

	plt.xlabel("x")
	plt.ylabel("y")
	plt.legend()
	plt.title("Original data and the fitted tanh curve")
	plt.ylim(-1.5, 1.5)
	plt.xlim(-1,6)
	plt.axhline(0, color= 'black')
	plt.axvline(0, color= 'black')
	plt.show()

#################### Task 2 ###################

"""
Compare the efficacy of the Softmax and
the Perceptron cost functions in terms of the
minimal number of misclassifications each can
achieve by proper minimization via gradient descent
on a breast cancer dataset.
"""
def perceptron(w, x, y):
    # Compute the predictions using the model
    predictions = model(x, w)

    # Calculate the ReLU-based loss element-wise
    relu_loss = jnp.maximum(0.0, -y * predictions)

    # Sum up the ReLU losses and average over the data points
    cost = jnp.sum(relu_loss) / float(y.size)

    return cost

def run_task2():
	# data input
	csvname = datapath + 'breast_cancer_data.csv'
	data = np.loadtxt(csvname,delimiter = ',')

	# get input and output of dataset
	x = data[:-1, :]
	y = data[-1:, :]

	print(np.shape(x)) # (8, 699)
	print(np.shape(y)) # (1, 699)

	# TODO: fill in the rest of the code
  # Hyperparameters
	max_iters = 2000
	alpha = 0.1

  # Initialize weights for Softmax and Perceptron
	w_softmax = jnp.zeros((9,1))  # Shape matches the number of features
	w_perceptron = jnp.zeros((9,1))  # Shape matches the number of features

	accuracy_history_softmax =[]
	accuracy_history_perceptron = []

  # Perform gradient descent for Softmax
	w_softmax, cost_history_softmax, accuracy_history_softmax = gradient_descent(x, y, w_softmax, alpha, max_iters, cost_function=softmax)

  # Calculate misclassifications and accuracy for Softmax
	misclassifications_softmax, accuracy_softmax = calculate_metrics(x, y, w_softmax)
  # Perform gradient descent for Perceptron
	w_perceptron, cost_history_perceptron, accuracy_history_perceptron = gradient_descent(x, y, w_perceptron, alpha, max_iters, cost_function=perceptron)
	plt.figure()
	plt.plot(range(len(cost_history_softmax)), cost_history_softmax, label='cost_history_softmax', color='b')
	plt.plot(range(len(cost_history_perceptron)), cost_history_perceptron, label='cost_history_perceptron', color='r')
	plt.title('Cost History Over Iterations')
	plt.xlabel('Iterations')
	plt.ylabel('Cost')
	plt.legend()
	plt.show()

	plt.plot(accuracy_history_perceptron, label='accuracy_history_perceptron', color='r')
	plt.plot(accuracy_history_softmax, label='accuracy_history_softmax', color='b')
	plt.title('Accuracy Over Iterations')
	plt.xlabel('Iterations')
	plt.ylabel('Accuracy')
	plt.legend()
	plt.show()
  # Calculate misclassifications and accuracy for Perceptron
	misclassifications_perceptron, accuracy_perceptron = calculate_metrics(x, y, w_perceptron)

	print("Softmax Misclassifications:", misclassifications_softmax)
	print("Softmax Accuracy: {:.2f}%".format(accuracy_softmax))

	print("Perceptron Misclassifications:", misclassifications_perceptron)
	print("Perceptron Accuracy: {:.2f}%".format(accuracy_perceptron))


if __name__ == '__main__':
	run_task1()
	run_task2()
