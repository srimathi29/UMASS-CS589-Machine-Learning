# -*- coding: utf-8 -*-
"""project_589.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DOtcqZgWS9bGh0ClNDbewcerZQLJ0ICN
"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from google.colab import drive

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

def load_data(train_path, test_path):
    """
    Load training and test datasets.

    Parameters:
    - train_path (str): Path to the training dataset.
    - test_path (str): Path to the test dataset.

    Returns:
    - pd.DataFrame: Training dataset.
    - pd.DataFrame: Test dataset.
    """
    train_data = pd.read_csv(train_path)
    test_data = pd.read_csv(test_path)
    return train_data, test_data

def preprocess_data(data, is_train=True):
    """
    Perform data preprocessing steps.

    Parameters:
    - data (pd.DataFrame): Input dataset.

    Returns:
    - pd.DataFrame: Processed feature matrix.
    - pd.Series: Target variable.
    """
    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch']
    X = data[features]


    # Convert categorical features to numerical using one-hot encoding
    X = pd.get_dummies(X, columns=['Sex'], drop_first=True)

    # Fill missing values in the 'Age' column
    X['Age'].fillna(X['Age'].median(), inplace=True)

    if is_train:
      return X, data['Survived']
    else:
      return X

def split_data(X, y):
    """
    Split data into training and validation sets.

    Parameters:
    - X (pd.DataFrame): Feature matrix.
    - y (pd.Series): Target variable.

    Returns:
    - pd.DataFrame: Training feature matrix.
    - pd.DataFrame: Validation feature matrix.
    - pd.Series: Training target variable.
    - pd.Series: Validation target variable.
    """
    return train_test_split(X, y, test_size=0.2, random_state=42)

def standardize_features(X_train, X_val):
    """
    Standardize features using StandardScaler.

    Parameters:
    - X_train (pd.DataFrame): Training feature matrix.
    - X_val (pd.DataFrame): Validation feature matrix.

    Returns:
    - pd.DataFrame: Standardized training feature matrix.
    - pd.DataFrame: Standardized validation feature matrix.
    """
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    return X_train_scaled, X_val_scaled

def train_svm(X_train, y_train):
    """
    Train a Support Vector Machine (SVM) model with hyperparameter tuning.

    Parameters:
    - X_train (pd.DataFrame): Standardized training feature matrix.
    - y_train (pd.Series): Training target variable.

    Returns:
    - Trained SVM model.
    """
    svm_param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 0.01, 0.001], 'kernel': ['rbf', 'linear', 'poly']}
    svm_model = GridSearchCV(SVC(), svm_param_grid, cv=3)
    svm_model.fit(X_train, y_train)
    return svm_model

def train_neural_network(X_train, y_train):
    """
    Train a Neural Network model with hyperparameter tuning.

    Parameters:
    - X_train (pd.DataFrame): Standardized training feature matrix.
    - y_train (pd.Series): Training target variable.

    Returns:
    - Trained Neural Network model.
    """
    nn_param_grid = {'hidden_layer_sizes': [(50,), (100,), (150,)], 'max_iter': [500, 1000, 1500]}
    nn_model = GridSearchCV(MLPClassifier(), nn_param_grid, cv=3)
    nn_model.fit(X_train, y_train)
    return nn_model

def train_random_forest(X_train, y_train):
    """
    Train a Random Forest model with hyperparameter tuning.

    Parameters:
    - X_train (pd.DataFrame): Standardized training feature matrix.
    - y_train (pd.Series): Training target variable.

    Returns:
    - Trained Random Forest model.
    """
    rf_param_grid = {'n_estimators': [50, 100, 150], 'max_depth': [None, 10, 20], 'min_samples_split': [2, 5, 10]}
    rf_model = GridSearchCV(RandomForestClassifier(), rf_param_grid, cv=3)
    rf_model.fit(X_train, y_train)
    return rf_model

def evaluate_model(model, X_val, y_val):
    """
    Evaluate a trained machine learning model.

    Parameters:
    - model: Trained machine learning model.
    - X_val (pd.DataFrame): Standardized validation feature matrix.
    - y_val (pd.Series): Validation target variable.

    Returns:
    - tuple: Evaluation metrics (accuracy, precision, recall, F1 score).
    """
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred) * 100
    precision = precision_score(y_val, y_pred) *100
    recall = recall_score(y_val, y_pred) *100

    return accuracy, precision, recall

def present_results(model_name, best_params, metrics):
    """
    Print results for a trained model.

    Parameters:
    - model_name (str): Name of the model.
    - best_params (dict): Best hyperparameters.
    - metrics (tuple): Evaluation metrics.
    """
    print(f"\n{model_name} Metrics:")
    print(f"Best hyperparameters: {best_params}")
    print("Accuracy: {:.2f}%".format(metrics[0]), "Precision: {:.2f}%".format(metrics[1]), "Recall: {:.2f}%".format(metrics[2]))

def plot_confusion_matrix(model, X_val, y_val, model_name):
    y_pred = model.predict(X_val)
    cm = confusion_matrix(y_val, y_pred)

   # plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])
    plt.title(f'Confusion Matrix - {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

def main():
    # Mount Google Drive
    drive.mount('/content/drive')
    datapath = '/content/drive/MyDrive/'

    # Load data
    train_data, test_data = load_data(datapath + 'train.csv', datapath + 'test.csv')

    # Data preprocessing
    X_train, y_train = preprocess_data(train_data)
    X_val = preprocess_data(test_data, is_train=False)

    # Split data
    X_train, X_val, y_train, y_val = split_data(X_train, y_train)

    # Standardize features
    X_train_scaled, X_val_scaled = standardize_features(X_train, X_val)

    # Model 1: Support Vector Machine
    svm_model = train_svm(X_train_scaled, y_train)

    # Model 2: Neural Network
    nn_model = train_neural_network(X_train_scaled, y_train)

    # Model 3: Random Forest
    rf_model = train_random_forest(X_train, y_train)

    # Model 1: Support Vector Machine
    svm_c_values = [param['C'] for param in svm_model.cv_results_['params']]
    svm_accuracy_scores = svm_model.cv_results_['mean_test_score']

    # Model 2: Neural Network
    nn_hidden_sizes = [param['hidden_layer_sizes'][0] for param in nn_model.cv_results_['params']]
    nn_accuracy_scores = nn_model.cv_results_['mean_test_score']

    # Model 3: Random Forest
    rf_n_estimators = [param['n_estimators'] for param in rf_model.cv_results_['params']]
    rf_accuracy_scores = rf_model.cv_results_['mean_test_score']

    # Evaluate models
    svm_metrics = evaluate_model(svm_model, X_val_scaled, y_val)
    nn_metrics = evaluate_model(nn_model, X_val_scaled, y_val)
    rf_metrics = evaluate_model(rf_model, X_val, y_val)

    # Plot confusion matrices for each model
    plot_confusion_matrix(svm_model, X_val_scaled, y_val, 'Support Vector Machine')
    plot_confusion_matrix(nn_model, X_val_scaled, y_val, 'Neural Network')
    plot_confusion_matrix(rf_model, X_val, y_val, 'Random Forest')

    # Print results
    present_results("Support Vector Machine", svm_model.best_params_, svm_metrics)
    present_results("Neural Network", nn_model.best_params_, nn_metrics)
    present_results("Random Forest", rf_model.best_params_, rf_metrics)


if __name__ == "__main__":
    main()