# -*- coding: utf-8 -*-
"""Hw5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12FshnXgSiWuDXSl9t5t-RoZzZXjZpvgk
"""

import jax.numpy as jnp
import numpy as np
from jax import grad
import matplotlib.pyplot as plt

#from google.colab import drive
#drive.mount('/content/drive')
#datapath = '/content/drive/MyDrive/'

datapath = "./"

#################### Task 3 ###################

"""
Implementing the multi-class classification with Softmax cost;
verify the implementation is correct by achiving small misclassification rate.
"""


# A helper function to plot the original data
def show_dataset(x, y):
  y = y.flatten()
  num_classes = np.size(np.unique(y.flatten()))
  accessible_color_cycle = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5']
  # initialize figure
  plt.figure()

  # color current class
  for a in range(0, num_classes):
    t = np.argwhere(y == a)
    t = t[:, 0]
    plt.scatter(
      x[0, t],
      x[1, t],
      s=50,
      color=accessible_color_cycle[a],
      edgecolor='k',
      linewidth=1.5,
      label="class:" + str(a))
    plt.xlabel("x1")
    plt.ylabel("x2")
    plt.legend(bbox_to_anchor=(1.1, 1.05))

  plt.show()
  #plt.savefig("data.png")
  #plt.close()

def show_dataset_labels(x, y, modelf, n_axis_pts=120):
  y = y.flatten()
  num_classes = np.size(np.unique(y.flatten()))
  accessible_color_cycle = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5']
  # initialize figure
  plt.figure()

  # fill in label regions using scatter points
  # get (x1, x2) for plot region
  anyax = np.linspace(0.05, 0.95, num=n_axis_pts)
  xx = np.meshgrid(anyax, anyax)
  xx_vars = np.reshape(xx, (2, n_axis_pts **2))
  temp = np.vstack([xx_vars, np.ones(xx_vars.shape[1])])
  #print(temp)
  z = modelf(temp)

  # get class label from model output
  y_hat = z.argmax(axis=1)

  for a in range(0, num_classes):
    t = np.argwhere(y_hat == a)
    t = t[:, 0]
    plt.scatter(
      xx_vars[0, t],
      xx_vars[1, t],
      s=5,
      color=accessible_color_cycle[a],
      linewidth=1.5,
      label="class:" + str(a))

  # color current class
  for a in range(0, num_classes):
    t = np.argwhere(y == a)
    t = t[:, 0]
    plt.scatter(
      x[0, t],
      x[1, t],
      s=50,
      color=accessible_color_cycle[a],
      edgecolor='k',
      linewidth=1.5,
      label="class:" + str(a))
    plt.xlabel("x1")
    plt.ylabel("x2")
  plt.legend(bbox_to_anchor=(1.1, 1.05))
  plt.show()
  #plt.savefig("classifier_label_regions.png")
  #plt.close()

# compute linear combination of input point
def model(x,w):
	a = jnp.dot(x.T,w)
	return a.T

lam = 10 **(-5)

def multiclass_softmax(w,x,y):
  all_evals = model(x,w)
  a = jnp.log(jnp.sum(jnp.exp(all_evals), axis = 0))
  b = all_evals[y.astype(int).flatten(),jnp.arange(jnp.size(y))]
  cost = jnp.sum(a-b)
  cost = cost + lam*jnp.linalg.norm(w[1:,:], 'fro')**2
  return cost/float(jnp.size(y))

def gradient_descent(x,y,w,alpha,iterations,cost_function):
	cost_history = []
	# print("gradient descent began")
	for i in range(iterations):
		# derivate of least squares is 1/m * (sum model(x,w) -y) x
		gradient = grad(cost_function)(w, x, y)
		w -= alpha * gradient
		cost = cost_function(w, x, y)
		cost_history.append(cost)
	return w, cost_history

def calculate_metrics(x, y, w_final):
		y_pred = model(x, w_final)
		predictions = jnp.argmax(y_pred, axis=0)
		misclassifications = jnp.sum((predictions) != y)
		accuracy = 100 - (misclassifications / y.size) * 100
		return misclassifications, accuracy

def run_task3():
  # load in dataset
  data = np.loadtxt(datapath + '4class_data.csv', delimiter=',')

  # get input/output pairs
  x = data[:-1, :]
  y = data[-1:, :]
  x = jnp.vstack([x, jnp.ones(x.shape[1])])

  print(np.shape(x))
  print(np.shape(y))

  show_dataset(x, y)

  # show data classified with dummy multiclass model
  def dummy_classifier_model(xs):
    y_hats = np.zeros((np.shape(xs)[1], 4))
    ys = ((1 - xs[0, :]) > xs[1, :]).astype(int)
    ys[np.where(xs[0,:] > xs[1,:])] = ys[np.where(xs[0,:] > xs[1,:])] + 2
    for i, e in enumerate(ys):
      y_hats[i,e] = 1
    return y_hats
  #show_dataset_labels(x, y, dummy_classifier_model)

  # TODO: fill in your code
  w_initial = np.random.randn(3,4)
  alpha = 1.0
  max_its = 2000
  w_final , cost_history = gradient_descent(x, y, w_initial, alpha, max_its, cost_function = multiclass_softmax)

 # Show the dataset with labeled regions
  show_dataset_labels(x[:-1,:], y, lambda x: jnp.dot(x.T,w_final))

  # Calculate and print the accuracy
  misclassifications, accuracy = calculate_metrics(x, y, w_final)
  print("Misclassifications:", misclassifications)
  print("Accuracy: {:.2f}%".format(accuracy))
  print("Final cost: ", cost_history[-1])

if __name__ == '__main__':
  run_task3()
