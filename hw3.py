# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16nN-uEyRbebRvg7GmuFkSPQn5ABxV1k1
"""

import jax.numpy as jnp
from jax import grad
# intro of jax library: https://jax.readthedocs.io/en/latest/notebooks/quickstart.html
import numpy as np
import matplotlib.pyplot as plt

#from google.colab import drive
#drive.mount('/content/drive')
#datapath = '/content/drive/MyDrive/'

datapath="./"

#plot function for graph
def compare_data(x, y,x_normalized_with_intercept, fitted_params1, fitted_params2):
	plt.scatter(x, y)
	plt.plot(x, model(x_normalized_with_intercept, fitted_params1), color='red', label='Least squares')
	plt.plot(x, model(x_normalized_with_intercept, fitted_params2), color='blue', label='Least deviation loss')
	plt.legend()
	plt.show()
	plt.show()

#plot function for graph
def plot_data(x, y,x_normalized_with_intercept, fitted_params):
	plt.scatter(x, y, label='Data')
	plt.plot(x, model(x_normalized_with_intercept, fitted_params), color='red', label='Fitted Line')
	plt.xlabel('Year')
	plt.ylabel('Student Debt')
	plt.legend()
	plt.title('Linear Regression of Student Debt')
	plt.show()

# Define the linear regression model
def model(x, w):
  return jnp.dot(x, w)

def denormalize_weights(w_normalized, x_original):
    w_original = [0, 0]
    w_original[0] = w_normalized[0] - np.sum(np.mean(x_original) * (w_normalized[1:] / np.std(x_original)))
    w_original[1] = w_normalized[1] / np.std(x_original)
    return w_original

def least_squares(w,x,y):
  cost = jnp.sum((model(x,w) - y)**2)
  return cost/float(y.size)

def gradient_least_squares(w,x,y):
	n = len(x)
	error = model(x,w) - y
	gradient = (1/n) * jnp.dot(x.T, error)
	return gradient

def least_deviation_loss(w,x,y):
	return jnp.mean(jnp.abs(model(x,w) - y))

def gradient_least_deviation_loss(w,x,y):
	m = len(x)
	gradients = jnp.zeros_like(w)

	for i in range(m):
		prediction = jnp.dot(x[i], w)
		diff = y[i] - prediction

    # Compute the derivative of absolute value
		if diff < 0:
			sign = -1
		elif diff > 0:
			sign = 1
		else:
			sign = 0

    # Update gradients
		gradients += sign * x[i]

  # Compute the gradient of the total mean absolute error
	gradients /= m
	return gradients

# Gradient Descent to fit the linear model
def gradient_descent(g, grad, x, y, w, alpha, num_iterations):
  for _ in range(num_iterations):
    gradient = grad(w,x,y)
    w -= alpha * gradient
  return w

#################### Task 1 ###################
"""
Fit a linear regression model to the student debt data
All parts marked "TO DO" are for you to construct.
"""
def run_task1():
	# import the dataset
	csvname = datapath + 'student_debt_data.csv'
	data = np.loadtxt(csvname,delimiter=',')

	# extract input - for this dataset, these are times
	x = data[:,0] #get all rows in the first column

	# extract output - for this dataset, these are total student debt
	y = data[:,1] #get all rows in the second column

	print(np.shape(x))
	print(np.shape(y))

	# TODO: fit a linear regression model to the data
	# Normalize the input feature (year)
	x_normalized = (x - jnp.mean(x)) / jnp.std(x)

	# Add a column of ones for the intercept term
	x_normalized_with_intercept = jnp.column_stack((jnp.ones_like(x_normalized), x_normalized))

	# Initialize parameters
	w_initial = jnp.ones(x_normalized_with_intercept.shape[1])

	# Hyperparameters
	alpha = 0.01
	num_iterations = 1000

	# Fit the linear regression model using gradient descent
	grad_least_squares = grad(least_squares, argnums= 0)
	fitted_params = gradient_descent(least_squares, grad_least_squares, x_normalized_with_intercept, y, w_initial, alpha, num_iterations)

	# Extract the intercept and slope from the fitted parameters
	m,c = denormalize_weights(fitted_params, x)
	plot_data(x, y,x_normalized_with_intercept, fitted_params)
	# Print the fitted line equation
	print(f"Equation of the fitted line: y = {c:}x + {m:}")

	# Predict student debt in 2030 (assuming the linear trend continues)
	future_year = 2030
	future_year_normalized = (future_year - jnp.mean(x)) / jnp.std(x)
	predicted_debt_2030 = model(jnp.array([1.0, future_year_normalized]), fitted_params)
	print(f"The total student debt in 2030 will be approximately ${predicted_debt_2030:}")

#################### Task 2 ###################

"""
Compare the least squares and the least absolute deviation costs
All parts marked "TO DO" are for you to construct.
"""

def run_task2():
	# load in dataset
	data = np.loadtxt(datapath + 'regression_outliers.csv',delimiter = ',')
	x = data[:-1,:]
	y = data[-1:,:]
	print(np.shape(x))
	print(np.shape(y))
	x = x[0]
	y = y[0]

  #Normalize the input feature (year)
	x_normalized = (x - jnp.mean(x)) / jnp.std(x)

	# Add a column of ones for the intercept term
	x_normalized_with_intercept = jnp.column_stack((jnp.ones_like(x_normalized), x_normalized))

	w_initial = jnp.ones(x_normalized_with_intercept.shape[1])

	alpha = 0.01
	num_iterations = 1000
	fitted_params1 = gradient_descent(least_squares, gradient_least_squares, x_normalized_with_intercept, y, w_initial, alpha, num_iterations)

	#calculate using second cost function
	grad_linear_deviation_loss = grad(least_deviation_loss,argnums=0)
	fitted_params2 = gradient_descent(least_deviation_loss, grad_linear_deviation_loss, x_normalized_with_intercept,y,w_initial, alpha, num_iterations)

	m1,c1 = denormalize_weights(fitted_params1, x)
	m2,c2 = denormalize_weights(fitted_params2, x)
	print(f"Equation of the fitted line with least squares: y = {c1:}x + {m1:}")
	print(f"Equation of the fitted line with least deviation loss: y = {c2:}x + {m2:}")
	compare_data(x, y,x_normalized_with_intercept, fitted_params1, fitted_params2)


if __name__ == '__main__':
	run_task1()
	run_task2()